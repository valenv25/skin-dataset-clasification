{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d80d810",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes con PyTorch y MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7f422",
   "metadata": {},
   "source": [
    "## Modelo Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ccca9e-4fec-44af-a8b7-d034101cb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f376548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "def train_model(device, model, train_loader, criterion, optimizer, epoch, n_epochs=10):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\tcorrect, total = 0, 0\n",
    "\n",
    "\tfor images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(images)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\t_, preds = torch.max(outputs, 1)\n",
    "\t\tcorrect += (preds == labels).sum().item()\n",
    "\t\ttotal += labels.size(0)\n",
    "\n",
    "\ttrain_loss = running_loss / len(train_loader)\n",
    "\ttrain_acc = 100.0 * correct / total\n",
    "\n",
    "\treturn train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9b8088-283f-4549-8c86-2fec28283e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación\n",
    "def evaluate_model(device, model, loader, criterion, writer, train_dataset, epoch=None, prefix=\"val\"):\n",
    "\tlog_classification_report(device, model, loader, writer, epoch, train_dataset, prefix)\n",
    "\tmodel.eval()\n",
    "\tcorrect, total, loss_sum = 0, 0, 0.0\n",
    "\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (images, labels) in enumerate(loader):\n",
    "\t\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\t_, preds = torch.max(outputs, 1)\n",
    "\n",
    "\t\t\tloss_sum += loss.item()\n",
    "\t\t\tcorrect += (preds == labels).sum().item()\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\n",
    "\t\t\tall_preds.extend(preds.cpu().numpy())\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\t\t\t# Loguear imágenes del primer batch\n",
    "\t\t\tif i == 0 and epoch is not None:\n",
    "\t\t\t\timg_grid = vutils.make_grid(images[:8].cpu(), normalize=True)\n",
    "\t\t\t\twriter.add_image(f\"{prefix}/images\", img_grid, global_step=epoch)\n",
    "\n",
    "\tacc = 100.0 * correct / total\n",
    "\tavg_loss = loss_sum / len(loader)\n",
    "\n",
    "\tif epoch is not None:\n",
    "\t\twriter.add_scalar(f\"{prefix}/loss\", avg_loss, epoch)\n",
    "\t\twriter.add_scalar(f\"{prefix}/accuracy\", acc, epoch)\n",
    "\n",
    "\treturn avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9baf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(device, model, writer, train_loader, val_loader, criterion, optimizer,\n",
    "                       train_dir, val_dir, train_dataset, n_epochs=10, batch_size=32):\n",
    "\twith mlflow.start_run():\n",
    "\t\t# Log hiperparámetros\n",
    "\t\tmlflow.log_params({\n",
    "\t\t\t\"model\": \"MLPClassifier\",\n",
    "\t\t\t\"input_size\": 64*64*3,\n",
    "\t\t\t\"batch_size\": batch_size,\n",
    "\t\t\t\"lr\": 1e-3,\n",
    "\t\t\t\"epochs\": n_epochs,\n",
    "\t\t\t\"optimizer\": \"Adam\",\n",
    "\t\t\t\"loss_fn\": \"CrossEntropyLoss\",\n",
    "\t\t\t\"train_dir\": train_dir,\n",
    "\t\t\t\"val_dir\": val_dir,\n",
    "\t\t})\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\ttrain_loss, train_acc = train_model(device, model, train_loader, criterion, optimizer, epoch, n_epochs)\n",
    "\t\t\tval_loss, val_acc = evaluate_model(device, model, val_loader, criterion, writer, train_dataset, epoch, prefix=\"val\")\n",
    "\n",
    "\t\t\tprint(f\"Epoch {epoch+1}:\")\n",
    "\t\t\tprint(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\t\t\tprint(f\"  Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "\t\t\twriter.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "\t\t\twriter.add_scalar(\"train/accuracy\", train_acc, epoch)\n",
    "\n",
    "\t\t\t# Log en MLflow\n",
    "\t\t\tmlflow.log_metrics({\n",
    "\t\t\t\t\"train_loss\": train_loss,\n",
    "\t\t\t\t\"train_accuracy\": train_acc,\n",
    "\t\t\t\t\"val_loss\": val_loss,\n",
    "\t\t\t\t\"val_accuracy\": val_acc\n",
    "\t\t\t}, step=epoch)\n",
    "\n",
    "\t\t# Guardar modelo\n",
    "\t\ttorch.save(model.state_dict(), \"mlp_model.pth\")\n",
    "\t\tprint(\"Modelo guardado como 'mlp_model.pth'\")\n",
    "\t\tmlflow.log_artifact(\"mlp_model.pth\")\n",
    "\t\tmlflow.pytorch.log_model(model, artifact_path=\"pytorch_model\")\n",
    "\t\tprint(\"Modelo guardado como 'mlp_model.pth'\")\n",
    "\n",
    "\treturn train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9d4d33-e68d-4ccb-8b0e-f18deabc659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo simple\n",
    "class MLPClassifier(nn.Module):\n",
    "\tdef __init__(self, input_size=64*64*3, num_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = nn.Sequential(\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(input_size, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, num_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05995607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_dir = \"../data/Split_smol/train\"\n",
    "val_dir = \"../data/Split_smol/val/\"\n",
    "\n",
    "# Transformaciones\n",
    "train_transform = A.Compose([\n",
    "\tA.Resize(64, 64),\n",
    "\tA.HorizontalFlip(p=0.5),\n",
    "\tA.RandomBrightnessContrast(p=0.2),\n",
    "\tA.Normalize(),\n",
    "\tToTensorV2()\n",
    "])\n",
    "\n",
    "val_test_transform = A.Compose([\n",
    "\tA.Resize(64, 64),\n",
    "\tA.Normalize(),\n",
    "\tToTensorV2()\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(train_dir, transform=train_transform)\n",
    "val_dataset   = CustomImageDataset(val_dir, transform=val_test_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08d5e76-139b-43b0-a5f0-a781b0a22463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 3.0769, Accuracy: 24.10%\n",
      "  Val   Loss: 2.1521, Accuracy: 34.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train Loss: 1.8779, Accuracy: 43.33%\n",
      "  Val   Loss: 1.7341, Accuracy: 43.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train Loss: 1.4187, Accuracy: 47.35%\n",
      "  Val   Loss: 1.4304, Accuracy: 50.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train Loss: 1.3427, Accuracy: 52.08%\n",
      "  Val   Loss: 1.2475, Accuracy: 53.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train Loss: 1.1715, Accuracy: 56.96%\n",
      "  Val   Loss: 1.3846, Accuracy: 51.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "  Train Loss: 1.0876, Accuracy: 59.83%\n",
      "  Val   Loss: 1.5588, Accuracy: 46.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "  Train Loss: 1.0860, Accuracy: 60.55%\n",
      "  Val   Loss: 1.2791, Accuracy: 59.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 22/22 [00:04<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "  Train Loss: 0.9512, Accuracy: 63.85%\n",
      "  Val   Loss: 1.1850, Accuracy: 54.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "  Train Loss: 0.9200, Accuracy: 65.14%\n",
      "  Val   Loss: 1.3597, Accuracy: 57.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "  Train Loss: 0.9669, Accuracy: 63.70%\n",
      "  Val   Loss: 1.2550, Accuracy: 53.59%\n",
      "Modelo guardado como 'mlp_model.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/21 21:59:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado como 'mlp_model.pth'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9669482057744806, 63.70157819225251, 1.2550140917301178, 53.591160220994475)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop de entrenamiento y validación\n",
    "n_epochs = 10\n",
    "\n",
    "# End any active MLflow run before starting a new one\n",
    "if mlflow.active_run() is not None:\n",
    "\tmlflow.end_run()\n",
    "mlflow.set_experiment(\"MLP_Clasificador_Imagenes\")\n",
    "\n",
    "# Crear directorio de logs\n",
    "log_dir = \"runs/mlp_experimento_1\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(set(train_dataset.labels))\n",
    "\n",
    "model = MLPClassifier(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_and_validate(device, model, writer, train_loader, val_loader, criterion, optimizer, train_dir, val_dir, train_dataset, n_epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ce1c4-58b3-4abf-8eb7-107b760ba69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPClassifier(num_classes=10)\n",
    "# model.load_state_dict(torch.load(\"mlp_model.pth\"))\n",
    "# model.eval()  # Para inferencia\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs/mlp_experimento_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d6d98",
   "metadata": {},
   "source": [
    "## Actividades de modificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68890dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifierComplete(nn.Module):\n",
    "    \"\"\"\n",
    "\tMLPClassifier con opciones avanzadas:\n",
    "    - Dropout\n",
    "\t- Batch Normalization\n",
    "    - Weight Initialization (He or Xavier)\n",
    "\t\"\"\"\n",
    "    def __init__(self, input_size=64*64*3, num_classes=10, use_dropout=False, dropout_p=0.5, use_batchnorm=False, init_type=None):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        \n",
    "        # Primera capa\n",
    "        layers.append(nn.Linear(input_size, 512))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(512))\n",
    "        layers.append(nn.ReLU())\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "        \n",
    "        # Segunda capa\n",
    "        layers.append(nn.Linear(512, 128))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers.append(nn.ReLU())\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "        \n",
    "        # Salida\n",
    "        layers.append(nn.Linear(128, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        if init_type is not None:\n",
    "            self.init_weights(init_type)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def init_weights(self, init_type):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if init_type == 'he':\n",
    "                    nn.init.kaiming_normal_(m.weight)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando variante: base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.82it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.82it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.98it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.95it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.95it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.92it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.94it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.97it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.93it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante base entrenada y guardada.\n",
      "\n",
      "Entrenando variante: dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.86it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.91it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.88it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  4.04it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  4.08it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.95it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.85it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.94it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.82it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante dropout entrenada y guardada.\n",
      "\n",
      "Entrenando variante: batchnorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.86it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.81it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.93it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  4.09it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.97it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.88it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.75it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.91it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  4.10it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante batchnorm entrenada y guardada.\n",
      "\n",
      "Entrenando variante: batchnorm_dropout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  4.11it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.96it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.92it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.99it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.89it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.78it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.89it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.97it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.81it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante batchnorm_dropout entrenada y guardada.\n",
      "\n",
      "Entrenando variante: weight_decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.70it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.80it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.83it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.94it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.99it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.97it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  4.00it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.67it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.85it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.89it/s]\n",
      "C:\\Users\\valen\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante weight_decay entrenada y guardada.\n",
      "\n",
      "Entrenando variante: augmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.82it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.78it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  3.87it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.83it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.84it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.82it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.81it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.70it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.77it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante augmentation entrenada y guardada.\n",
      "\n",
      "Entrenando variante: init_he\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.73it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.91it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  4.07it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.92it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  4.12it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  4.08it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.72it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.99it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  4.07it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante init_he entrenada y guardada.\n",
      "\n",
      "Entrenando variante: init_xavier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  3.84it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  4.02it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  4.04it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  4.03it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:05<00:00,  3.86it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:05<00:00,  3.80it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.85it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.92it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.91it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante init_xavier entrenada y guardada.\n",
      "\n",
      "Entrenando variante: histogramas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 22/22 [00:05<00:00,  4.00it/s]\n",
      "Epoch 2/10: 100%|██████████| 22/22 [00:05<00:00,  3.87it/s]\n",
      "Epoch 3/10: 100%|██████████| 22/22 [00:05<00:00,  4.02it/s]\n",
      "Epoch 4/10: 100%|██████████| 22/22 [00:05<00:00,  3.96it/s]\n",
      "Epoch 5/10: 100%|██████████| 22/22 [00:06<00:00,  3.47it/s]\n",
      "Epoch 6/10: 100%|██████████| 22/22 [00:06<00:00,  3.53it/s]\n",
      "Epoch 7/10: 100%|██████████| 22/22 [00:05<00:00,  3.74it/s]\n",
      "Epoch 8/10: 100%|██████████| 22/22 [00:05<00:00,  3.97it/s]\n",
      "Epoch 9/10: 100%|██████████| 22/22 [00:05<00:00,  3.98it/s]\n",
      "Epoch 10/10: 100%|██████████| 22/22 [00:05<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variante histogramas entrenada y guardada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variantes = [\n",
    "\t\"simple\",\n",
    "\t# \"pocas_imagenes\",\n",
    "\t# \"grises\",\n",
    "\t\"dropout\",\n",
    "\t\"batchnorm\",\n",
    "\t\"batchnorm_dropout\",\n",
    "\t\"weight_decay\",\n",
    "\t\"augmentation\",\n",
    "\t\"init_he\",\n",
    "\t\"init_xavier\",\n",
    "\t\"histogramas\"\n",
    "]\n",
    "\n",
    "# Loop de entrenamiento y validación\n",
    "n_epochs = 10\n",
    "\n",
    "# End any active MLflow run before starting a new one\n",
    "if mlflow.active_run() is not None:\n",
    "\tmlflow.end_run()\n",
    "mlflow.set_experiment(\"MLP_Clasificador_Imagenes_Adv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(set(train_dataset.labels))\n",
    "\n",
    "for var in variantes:\n",
    "\tprint(f\"Entrenando variante: {var}\")\n",
    "\t\n",
    "\tif var == \"base\" or var == \"weight_decay\" or var == \"augmentation\" or var == \"histogramas\":\n",
    "\t\tmodel = MLPClassifierComplete(num_classes=num_classes, use_dropout=False, use_batchnorm=False).to(device)\n",
    "\t# elif var == \"grises\":\n",
    "\t# elif var == \"pocas_imagenes\":\n",
    "\telif var == \"init_he\":\n",
    "\t\tmodel = MLPClassifierComplete(init_type='he', num_classes=num_classes).to(device)\n",
    "\telif var == \"init_xavier\":\n",
    "\t\tmodel = MLPClassifierComplete(init_type='xavier', num_classes=num_classes).to(device)\n",
    "\telif var == \"dropout\":\n",
    "\t\tmodel = MLPClassifierComplete(num_classes=num_classes, use_dropout=True, dropout_p=0.5).to(device)\n",
    "\telif var == \"batchnorm\":\n",
    "\t\tmodel = MLPClassifierComplete(num_classes=num_classes, use_batchnorm=True).to(device)\n",
    "\telif var == \"batchnorm_dropout\":\n",
    "\t\tmodel = MLPClassifierComplete(num_classes=num_classes, use_dropout=True, dropout_p=0.5, use_batchnorm=True).to(device)\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Variante '{var}' no reconocida.\")\n",
    "\n",
    "\tif var == \"weight_decay\":\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\telse:\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\tif var == \"augmentation\":\n",
    "\t\ttrain_transform_adv = A.Compose([\n",
    "\t\t\tA.Resize(64, 64),\n",
    "\t\t\tA.HorizontalFlip(p=0.5),\n",
    "\t\t\t# A.VerticalFlip(p=0.5),\n",
    "\t\t\tA.RandomBrightnessContrast(p=0.2),\n",
    "\t\t\tA.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "\t\t\t# A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "\t\t\tA.Normalize(),\n",
    "\t\t\tToTensorV2()\n",
    "\t\t])\n",
    "\t\ttrain_dataset_adv = CustomImageDataset(train_dir, transform=train_transform_adv)\n",
    "\t\ttrain_loader_t = DataLoader(train_dataset_adv, batch_size=batch_size, shuffle=True)\n",
    "\telse:\n",
    "\t\ttrain_loader_t = train_loader\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\twriter = SummaryWriter(log_dir=f\"runs/experimento_{var}\")\n",
    "\n",
    "\twith mlflow.start_run(run_name=f\"MLP_{var}\"):\n",
    "\t\t# Log hiperparámetros\n",
    "\t\tmlflow.log_params({\n",
    "\t\t\t\"model\": \"MLPClassifierComplete\",\n",
    "\t\t\t\"input_size\": 64*64*3,\n",
    "\t\t\t\"batch_size\": batch_size,\n",
    "\t\t\t\"lr\": 1e-3,\n",
    "\t\t\t\"epochs\": n_epochs,\n",
    "\t\t\t\"optimizer\": \"Adam\",\n",
    "\t\t\t\"loss_fn\": \"CrossEntropyLoss\",\n",
    "\t\t\t\"train_dir\": train_dir,\n",
    "\t\t\t\"val_dir\": val_dir,\n",
    "\t\t})\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\ttrain_loss, train_acc = train_model(device, model, train_loader, criterion, optimizer, epoch, n_epochs)\n",
    "\t\t\tval_loss, val_acc = evaluate_model(device, model, val_loader, criterion, writer, train_dataset, epoch, prefix=\"val\")\n",
    "\n",
    "\t\t\twriter.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "\t\t\twriter.add_scalar(\"train/accuracy\", train_acc, epoch)\n",
    "\n",
    "\t\t\t# Log en MLflow\n",
    "\t\t\tmlflow.log_metrics({\n",
    "\t\t\t\t\"train_loss\": train_loss,\n",
    "\t\t\t\t\"train_accuracy\": train_acc,\n",
    "\t\t\t\t\"val_loss\": val_loss,\n",
    "\t\t\t\t\"val_accuracy\": val_acc\n",
    "\t\t\t}, step=epoch)\n",
    "\t\t\tif var == \"histogramas\":\n",
    "\t\t\t\t# Histogramas\n",
    "\t\t\t\tfor name, param in model.named_parameters():\n",
    "\t\t\t\t\twriter.add_histogram(name, param, epoch)\n",
    "\t\t\n",
    "\t\ttorch.save(model.state_dict(), f\"models/mlp_{var}.pth\")\n",
    "\t\tmlflow.log_param(\"variante\", var)\n",
    "\t\tmlflow.log_artifact(f\"models/mlp_{var}.pth\")\n",
    "\t\n",
    "\twriter.close()\n",
    "\tprint(f\"Variante {var} entrenada y guardada.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow      -> http://localhost:5000\n",
    "# tensorboard -> http://localhost:6006\n",
    "\n",
    "!mlflow ui\n",
    "# load_ext tensorboard\n",
    "!tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
